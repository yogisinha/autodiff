{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff demo\n",
    "\n",
    "Following is the demo for small library for Reverse-mode Automatic Differentiation, to be used in Neural network training. I wrote this as a solution of one of the assignments in Deep learning course from University of Washington. https://github.com/dlsys-course/assignment1. I am not enrolled in this course. This was only for learning purpose.\n",
    "\n",
    "This notebook demos the comparison between this library and Tensorflow library in training of a small neural network doing the classification of images among 3 classes. It demos how the loss decreases after each epoch of training and results almost matches with Tensorflow execution.\n",
    "\n",
    "**That assignment had skeleton code for:**\n",
    "- Node, Op and Executor classes (run and gradients method)\n",
    "- Implementions for AddOp, AddByConstOp\n",
    "\n",
    "**I Implemented:**\n",
    "- Operations like \n",
    "    - SubOp, MulOp, AddByConstOp, MulByConstOp, PlaceholderOp   \n",
    "    - MatMulOp, TransposeOp    \n",
    "    - OnesLikeOp, ZerosLikeOp\n",
    "    - ExpXOp, InverseOp, ReduceMeanOp\n",
    "    - SizeOp, LogOp, AssignOp\n",
    "    - SoftmaxOp, Softmax_With_Cross_EntropyOp\n",
    "    - SigmoidOp, Sigmoid_With_Cross_EntropyOp\n",
    "    - ReluOp, Relu_DerivativeOp\n",
    "- Gradient computation of trainable variable w.r.t. cost\n",
    "- Executor.run method which computes the values for a given subset of nodes in a computation graph.\n",
    "- Optimizer such as GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from pathlib import Path, PurePath, WindowsPath\n",
    "from operator import itemgetter\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, convert_to_one_hot, predict\n",
    "\n",
    "from image_dataset_load import *\n",
    "\n",
    "# Importing autodiff library\n",
    "import autodiff as ad\n",
    "import train as tr  # this module has GradientDescentOptimizer class\n",
    "import initializers as init\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image(data, label, ind, dimension, print_class=False):\n",
    "    w, h = dimension\n",
    "    test_image = data[:, ind].reshape(h, w, 3)\n",
    "    test_label = label[:, ind]    \n",
    "    if print_class:\n",
    "        print(test_label, np.argmax(test_label))\n",
    "    plt.imshow(test_image)\n",
    "    plt.show()\n",
    "    return np.argmax(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the 3 class of images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 80, 80, 3)\n",
      "(653, 80, 80, 3)\n",
      "(896, 80, 80, 3)\n",
      "train_X shape, train_Y shape  (19200, 700) (3, 700)\n",
      "test_X shape, test_Y shape  (19200, 196) (3, 196)\n"
     ]
    }
   ],
   "source": [
    "#X, Y = load_img_dataset()\n",
    "X, Y = loadImgsLabels([(r\"test\\size_80x80\\1\", 0), (r\"test\\size_80x80\\2\", 1), (r\"test\\size_80x80\\3\", 2)], (80, 80))\n",
    "X, Y = randomize_std(X, Y)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = split_train_test(X, Y, 700)\n",
    "\n",
    "train_Y = one_hot(train_Y, num_classes)\n",
    "test_Y = one_hot(test_Y, num_classes)\n",
    "\n",
    "train_Y = train_Y.reshape((num_classes, train_Y.shape[2]))\n",
    "test_Y = test_Y.reshape((num_classes, test_Y.shape[2]))\n",
    "\n",
    "print(\"train_X shape, train_Y shape \", train_X.shape, train_Y.shape)\n",
    "print(\"test_X shape, test_Y shape \", test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, batch_size=10, seed=0):    \n",
    "    \"\"\"\n",
    "    Shuffle the input X and produce mini batches in size of batch_size\n",
    "    \n",
    "    Arguments:\n",
    "    X - input.  dimension ( input features, no of examples )\n",
    "    Y - labels\n",
    "    batch_size - size of the mini-batch\n",
    "    seed - seed to randomize the X before splitting it into mini-batches\n",
    "\n",
    "    Returns:\n",
    "    batches of X and Y split into size of batch_size\n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    permute = list(np.random.permutation(m))\n",
    "    \n",
    "    X = X[:, permute]\n",
    "    Y = Y[:, permute]     \n",
    "    \n",
    "    \n",
    "    a = list(range(0, m, batch_size))\n",
    "    b = list(range(batch_size, m, batch_size))    \n",
    "    b.append(m)\n",
    "    \n",
    "    for (start, end) in zip(a, b):        \n",
    "        yield X[:, start:end], Y[:, start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the training variables, computaion graph and training the model using **autodiff** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.005\n",
    "\n",
    "W1 = ad.Variable(name = \"W1\", shape = (100, train_X.shape[0]), initializer = init.xavier_initializer())\n",
    "W2 = ad.Variable(name = \"W2\", shape = (20, 100), initializer = init.xavier_initializer())\n",
    "W3 = ad.Variable(name = \"W3\", shape = (3, 20), initializer = init.xavier_initializer())\n",
    "\n",
    "x, labels = ad.placeholder(name = \"x\"), ad.placeholder(name = \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_vars  [W1, W2, W3]\n",
      "Cost after epoch 1: 12.821472\n",
      "Cost after epoch 2: 10.853068\n",
      "Cost after epoch 3: 8.807595\n",
      "Cost after epoch 4: 8.930385\n",
      "Cost after epoch 5: 8.021197\n",
      "Cost after epoch 6: 7.290488\n",
      "Cost after epoch 7: 7.405478\n",
      "Cost after epoch 8: 6.796811\n",
      "Cost after epoch 9: 6.569471\n",
      "Cost after epoch 10: 5.840512\n",
      "Cost after epoch 11: 6.227665\n",
      "Cost after epoch 12: 5.440793\n",
      "Cost after epoch 13: 5.574930\n",
      "Cost after epoch 14: 5.067640\n",
      "Cost after epoch 15: 4.978729\n",
      "Cost after epoch 16: 4.560718\n",
      "Cost after epoch 17: 4.667846\n",
      "Cost after epoch 18: 4.854133\n",
      "Cost after epoch 19: 4.834579\n",
      "Cost after epoch 20: 4.066814\n",
      "Cost after epoch 21: 4.006650\n",
      "Cost after epoch 22: 3.910609\n",
      "Cost after epoch 23: 3.240038\n",
      "Cost after epoch 24: 3.501762\n",
      "Cost after epoch 25: 2.805002\n",
      "Cost after epoch 26: 3.898406\n",
      "Cost after epoch 27: 3.033619\n",
      "Cost after epoch 28: 2.814546\n",
      "Cost after epoch 29: 2.495004\n",
      "Cost after epoch 30: 2.709383\n",
      "Cost after epoch 31: 2.337458\n",
      "Cost after epoch 32: 1.938666\n",
      "Cost after epoch 33: 1.658287\n",
      "Cost after epoch 34: 2.566452\n",
      "Cost after epoch 35: 2.302448\n",
      "Cost after epoch 36: 1.592777\n",
      "Cost after epoch 37: 1.748373\n",
      "Cost after epoch 38: 1.792497\n",
      "Cost after epoch 39: 0.943374\n",
      "Cost after epoch 40: 0.793969\n",
      "Cost after epoch 41: 0.880805\n",
      "Cost after epoch 42: 1.090151\n",
      "Cost after epoch 43: 0.530984\n",
      "Cost after epoch 44: 0.413354\n",
      "Cost after epoch 45: 0.375753\n",
      "Cost after epoch 46: 0.344853\n",
      "Cost after epoch 47: 0.338764\n",
      "Cost after epoch 48: 0.280791\n",
      "Cost after epoch 49: 0.280215\n",
      "Cost after epoch 50: 0.287782\n"
     ]
    }
   ],
   "source": [
    "Z1 = ad.matmul(W1, x)\n",
    "A1 = ad.relu(Z1)\n",
    "Z2 = ad.matmul(W2, A1)\n",
    "A2 = ad.relu(Z2)\n",
    "Z3 = ad.matmul(W3, A2)\n",
    "\n",
    "cost = ad.reduce_mean( ad.softmax_with_cross_entropy(Z3, labels) )\n",
    "optimizer = tr.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "optimizer.append(cost)\n",
    "executor = ad.Executor(optimizer)\n",
    "\n",
    "seed = 0\n",
    "costs = []   # to track the costs\n",
    "epochs = 50\n",
    "mini_batch_size = 10\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    epoch_cost = 0\n",
    "    seed += 1\n",
    "    \n",
    "    for (batch_X, batch_Y) in random_mini_batches(train_X, train_Y, mini_batch_size, seed):\n",
    "        curr_batch_size = batch_X.shape[1]\n",
    "        \n",
    "        _, minibatch_cost = executor.run(feed_dict = {x : batch_X, labels : batch_Y})        \n",
    "        epoch_cost += minibatch_cost/curr_batch_size\n",
    "        \n",
    "    print(\"Cost after epoch %i: %f\" % (epoch+1, epoch_cost))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the training variables, computaion graph and training the model using **Tensorflow** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(num_features, classes):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for training data X and traininig class labels Y\n",
    "                     \n",
    "    Arguments:\n",
    "    num_inputs -- no. of input features \n",
    "    classes -- number of classes\n",
    "    \n",
    "    Returns: \n",
    "    X and Y placeholders\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(num_features, None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(classes, None), name='Y')\n",
    "    \n",
    "    return X, Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(num_features, classes):\n",
    "    \"\"\"\n",
    "    initialize the wts and biases for the network\n",
    "    \n",
    "    Arguments:\n",
    "    num_inputs -- no. of input features \n",
    "    classes -- number of classes\n",
    "    \n",
    "    Returns :\n",
    "    dictionary of tensors containing W1, b1, W2, b2, W3, b3    \n",
    "    \"\"\"\n",
    "    \n",
    "    random_init = tf.random_normal_initializer()\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer(seed = 1)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [100, num_features], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [300, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", [20, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [100, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", [classes, 20], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [classes, 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\":W1, \"b1\":b1,\n",
    "                  \"W2\":W2, \"b2\":b2,\n",
    "                  \"W3\":W3, \"b3\":b3\n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    Computes the forward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of dimension ( input features, no of examples )\n",
    "    parameters -- training wts and biases\n",
    "    \n",
    "    Returns: \n",
    "    linear output tensor from last unit   \n",
    "    \"\"\"\n",
    "    \n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "    W3, b3 = parameters[\"W3\"], parameters[\"b3\"]\n",
    "\n",
    "    Z1 = tf.matmul(W1, X) #+ b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.matmul(W2, A1) #+ b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.matmul(W3, A2) #+ b3\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y, l2_regularize, beta, parameters):\n",
    "    \"\"\"\n",
    "    computes the softmax cross entropy cost\n",
    "\n",
    "    Arguments :\n",
    "    Z3 - linear output from last unit of network (classes, batch size)\n",
    "    Y -  one hot encoding of labels (classes, batch size)\n",
    "\n",
    "    Returns :\n",
    "    Tensor of the cost function \n",
    "    \"\"\"\n",
    "\n",
    "    # to fit the requirement of tf.nn.softmax_cross_entropy_with_logits... . this function expects the \n",
    "    # inputs in dimension (batch size, classes)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    regularizer = 0\n",
    "    if l2_regularize:\n",
    "        W1 = parameters[\"W1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        W3 = parameters[\"W3\"]\n",
    "        \n",
    "        regularizer = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)        \n",
    "        \n",
    "\n",
    "    cost = tf.reduce_mean( \n",
    "          tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels) + beta*regularizer )\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(Z3, Y1, train_X, train_Y, test_X, test_Y):\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy1.eval({X: train_X, Y: train_Y}))\n",
    "    print (\"Test Accuracy:\", accuracy1.eval({X: test_X, Y: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_X, train_Y, dev_X, dev_Y, test_X, test_Y, learning_rate = 0.005, \n",
    "          num_epochs = 500, mini_batch_size = 10, \n",
    "          l2_regularize=False, beta=0.01):\n",
    "    \n",
    "    print(\"learning_rate %f, num_epochs %i, mini_batch_size %i, l2_regularize %s, beta %f\" % \n",
    "          (learning_rate, num_epochs, mini_batch_size, l2_regularize, beta))\n",
    "\n",
    "\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    num_features, classes = train_X.shape[0], train_Y.shape[0]\n",
    "\n",
    "    X, Y = create_placeholders(num_features, classes)\n",
    "\n",
    "    parameters = initialize_parameters(num_features, classes)\n",
    "\n",
    "    Z3 = forward_prop(X, parameters)\n",
    "\n",
    "    cost = compute_cost(Z3, Y, l2_regularize, beta, parameters)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # train \n",
    "    seed = 0\n",
    "    costs = []   # to track the costs\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0\n",
    "            seed += 1\n",
    "\n",
    "            for (batch_X, batch_Y) in random_mini_batches(train_X, train_Y, mini_batch_size, seed):\n",
    "                curr_batch_size = batch_X.shape[1]\n",
    "\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict = {X:batch_X, Y:batch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost/curr_batch_size\n",
    "\n",
    "            print(\"Cost after epoch %i: %f\" % (epoch+1, epoch_cost))\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        print (\"Test Accuracy: \", accuracy.eval({X: dev_X, Y: dev_Y}))\n",
    "        \n",
    "    \n",
    "    prediction = tf.argmax(Z3)       \n",
    "    \n",
    "    return parameters, prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate 0.005000, num_epochs 50, mini_batch_size 10, l2_regularize False, beta 0.020000\n",
      "WARNING:tensorflow:From <ipython-input-11-c4c1b8be4a10>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Cost after epoch 1: 7.061410\n",
      "Cost after epoch 2: 5.622121\n",
      "Cost after epoch 3: 4.789546\n",
      "Cost after epoch 4: 4.786476\n",
      "Cost after epoch 5: 4.442242\n",
      "Cost after epoch 6: 3.997604\n",
      "Cost after epoch 7: 4.208305\n",
      "Cost after epoch 8: 3.738872\n",
      "Cost after epoch 9: 3.597192\n",
      "Cost after epoch 10: 3.218843\n",
      "Cost after epoch 11: 3.436869\n",
      "Cost after epoch 12: 2.977800\n",
      "Cost after epoch 13: 2.980100\n",
      "Cost after epoch 14: 2.843598\n",
      "Cost after epoch 15: 2.752924\n",
      "Cost after epoch 16: 2.586309\n",
      "Cost after epoch 17: 2.521039\n",
      "Cost after epoch 18: 2.845722\n",
      "Cost after epoch 19: 2.630410\n",
      "Cost after epoch 20: 2.343413\n",
      "Cost after epoch 21: 2.231158\n",
      "Cost after epoch 22: 2.070470\n",
      "Cost after epoch 23: 1.799476\n",
      "Cost after epoch 24: 1.893818\n",
      "Cost after epoch 25: 1.453050\n",
      "Cost after epoch 26: 2.118791\n",
      "Cost after epoch 27: 1.670438\n",
      "Cost after epoch 28: 1.728051\n",
      "Cost after epoch 29: 1.353458\n",
      "Cost after epoch 30: 1.494703\n",
      "Cost after epoch 31: 1.211166\n",
      "Cost after epoch 32: 0.990121\n",
      "Cost after epoch 33: 1.095266\n",
      "Cost after epoch 34: 1.154167\n",
      "Cost after epoch 35: 0.952598\n",
      "Cost after epoch 36: 1.119635\n",
      "Cost after epoch 37: 1.100211\n",
      "Cost after epoch 38: 0.656051\n",
      "Cost after epoch 39: 0.486208\n",
      "Cost after epoch 40: 0.453792\n",
      "Cost after epoch 41: 1.525217\n",
      "Cost after epoch 42: 0.397691\n",
      "Cost after epoch 43: 0.281522\n",
      "Cost after epoch 44: 0.314872\n",
      "Cost after epoch 45: 0.235199\n",
      "Cost after epoch 46: 0.200561\n",
      "Cost after epoch 47: 0.180143\n",
      "Cost after epoch 48: 0.159273\n",
      "Cost after epoch 49: 0.146227\n",
      "Cost after epoch 50: 0.148467\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+P82iDl4oVTClQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c97a978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.80102\n"
     ]
    }
   ],
   "source": [
    "parameters, prediction = train(train_X, train_Y, test_X, test_Y, None, None,\n",
    "                   num_epochs=50, learning_rate = 0.005, \n",
    "                   mini_batch_size = 10, l2_regularize=False, beta=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
